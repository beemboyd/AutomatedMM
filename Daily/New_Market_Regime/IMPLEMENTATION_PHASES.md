# Market Regime ML - Detailed Implementation Phases

## Phase 1: Data Pipeline Foundation ðŸ”„
**Timeline**: Days 1-3
**Goal**: Establish reliable data flow

### 1.1 Data Ingestor
```python
# src/ingestion/data_ingestor.py
class MarketDataIngestor:
    def __init__(self):
        self.scanner_results_path = "../results/"
        self.market_data_path = "./data/raw/"
        
    def ingest_scanner_data(self):
        """Collect scanner results every 5 minutes"""
        # Read Long/Short reversal scanner results
        # Read KC scanner results
        # Store with timestamp
        
    def ingest_market_data(self):
        """Collect NIFTY, sectoral indices data"""
        # Get index prices
        # Get breadth metrics
        # Store in structured format
        
    def schedule_ingestion(self):
        """Run every 5 minutes during market hours"""
        # 9:15 AM to 3:30 PM IST
```

### 1.2 Data Validator
```python
# src/validation/data_validator.py
class DataValidator:
    def __init__(self):
        self.validation_rules = {
            'price': {'min': 0, 'max': 100000},
            'volume': {'min': 0, 'max': float('inf')},
            'rsi': {'min': 0, 'max': 100}
        }
        
    def clean_data(self, df):
        """Remove invalid values"""
        # Handle inf, -inf, NaN
        # Check for access token errors (all zeros)
        # Validate ranges
        
    def enforce_schema(self, df):
        """Ensure consistent structure"""
        # Required columns
        # Data types
        # Missing value handling
```

### 1.3 Configuration
```yaml
# config/config.yaml
data_pipeline:
  ingestion_interval: 300  # seconds
  market_hours:
    start: "09:15"
    end: "15:30"
  validation:
    remove_outliers: true
    outlier_threshold: 3  # standard deviations
    min_data_points: 50
```

**Deliverables**:
- âœ… Automated data collection every 5 minutes
- âœ… Clean, validated dataset
- âœ… Data quality reports

---

## Phase 2: Feature Engineering ðŸ”§
**Timeline**: Days 4-7
**Goal**: Build reusable feature pipeline

### 2.1 Feature Builder
```python
# src/features/feature_builder.py
class FeatureBuilder:
    def __init__(self):
        self.feature_definitions = self.load_feature_config()
        
    def build_price_features(self, df):
        """Price-based features"""
        features = {}
        features['returns_5m'] = df['close'].pct_change()
        features['volatility_1h'] = df['close'].rolling(12).std()
        features['price_momentum'] = df['close'] - df['close'].shift(20)
        return features
        
    def build_technical_features(self, df):
        """Technical indicators"""
        features['rsi'] = calculate_rsi(df['close'])
        features['macd'] = calculate_macd(df['close'])
        features['bb_position'] = bollinger_position(df['close'])
        return features
        
    def build_breadth_features(self, scanner_data):
        """Market breadth metrics"""
        features['long_short_ratio'] = scanner_data['long_count'] / scanner_data['short_count']
        features['bullish_percent'] = scanner_data['bullish'] / scanner_data['total']
        return features
```

### 2.2 Feature Store
```python
# src/features/feature_store.py
class FeatureStore:
    def __init__(self):
        self.store_path = "./data/features/"
        self.feature_version = "v1.0"
        
    def save_features(self, features, timestamp):
        """Store features with versioning"""
        # Save to parquet for efficiency
        # Include timestamp and version
        
    def load_features(self, start_date, end_date):
        """Load features for training/serving"""
        # Consistent loading logic
        # Handle missing data
```

### 2.3 Regime Labeler
```python
# src/features/regime_labeler.py
class RegimeLabeler:
    def __init__(self):
        self.regime_definitions = {
            'strongly_bullish': lambda x: x['returns'] > 0.015 and x['volatility'] < 0.02,
            'bullish': lambda x: x['returns'] > 0.005,
            'neutral': lambda x: abs(x['returns']) < 0.005,
            'bearish': lambda x: x['returns'] < -0.005,
            'strongly_bearish': lambda x: x['returns'] < -0.015 and x['volatility'] < 0.02,
            'choppy_bullish': lambda x: x['returns'] > 0 and x['volatility'] > 0.02,
            'choppy_bearish': lambda x: x['returns'] < 0 and x['volatility'] > 0.02
        }
        
    def label_regimes(self, df):
        """Apply regime labels to historical data"""
        # Calculate regime for each period
        # Handle regime transitions
        # Store labeled data
```

**Deliverables**:
- âœ… 30+ engineered features
- âœ… Feature store with versioning
- âœ… Labeled training dataset

---

## Phase 3: Model Training & Registry ðŸ¤–
**Timeline**: Days 8-10
**Goal**: Build and register models

### 3.1 Model Trainer
```python
# src/training/model_trainer.py
class ModelTrainer:
    def __init__(self):
        self.model_type = "RandomForest"
        self.hyperparameters = self.load_hyperparameters()
        
    def train(self, X_train, y_train):
        """Train model with best practices"""
        # Time-based split (80/20)
        # Cross-validation
        # Hyperparameter tuning
        # Feature selection
        
    def create_baseline(self):
        """Simple baseline for comparison"""
        # Persistence model (previous regime continues)
        # Moving average based model
```

### 3.2 Model Registry
```python
# src/training/model_registry.py
class ModelRegistry:
    def __init__(self):
        self.registry_path = "./models/registry.json"
        
    def register_model(self, model, metadata):
        """Save model with metadata"""
        metadata = {
            'version': 'v1.0',
            'timestamp': datetime.now(),
            'accuracy': 0.87,
            'features_used': [...],
            'training_data': 'data_v1',
            'git_commit': get_git_hash()
        }
        # Save model artifact
        # Update registry
        
    def load_model(self, version='latest'):
        """Load specific model version"""
        # Retrieve from registry
        # Load model artifact
```

**Deliverables**:
- âœ… Trained Random Forest model
- âœ… Model registry with versioning
- âœ… Performance metrics vs baseline

---

## Phase 4: Model Serving ðŸš€
**Timeline**: Days 11-14
**Goal**: Deploy predictions

### 4.1 Batch Predictor
```python
# src/serving/batch_predictor.py
class BatchPredictor:
    def __init__(self):
        self.model = ModelRegistry().load_model('latest')
        self.feature_builder = FeatureBuilder()
        
    def predict_hourly(self):
        """Run hourly predictions"""
        # Load latest data
        # Build features (same as training)
        # Generate predictions
        # Store results
```

### 4.2 API Server
```python
# src/serving/api_server.py
from flask import Flask, jsonify

app = Flask(__name__)

@app.route('/predict', methods=['GET'])
def predict():
    """Real-time prediction endpoint"""
    # Load latest features
    # Run prediction
    # Return JSON response
    return jsonify({
        'regime': 'bullish',
        'confidence': 0.85,
        'timestamp': datetime.now()
    })

@app.route('/dashboard', methods=['GET'])
def dashboard():
    """Web dashboard on port 8080"""
    # Display current regime
    # Show confidence
    # Historical predictions
```

**Deliverables**:
- âœ… REST API on port 8080
- âœ… Hourly batch predictions
- âœ… Web dashboard

---

## Phase 5: Monitoring & Auto-Adaptation ðŸ”„
**Timeline**: Days 15-20
**Goal**: Self-improving system

### 5.1 Drift Detector
```python
# src/monitoring/drift_detector.py
class DriftDetector:
    def __init__(self):
        self.baseline_distributions = self.load_baseline()
        
    def detect_feature_drift(self, current_features):
        """Monitor feature distributions"""
        # KS test for distribution changes
        # Track mean/std shifts
        # Alert on significant drift
        
    def detect_prediction_drift(self, predictions):
        """Monitor prediction patterns"""
        # Check regime distribution
        # Detect monoculture
        # Track confidence changes
```

### 5.2 Auto Retrainer
```python
# src/monitoring/auto_retrainer.py
class AutoRetrainer:
    def __init__(self):
        self.retrain_threshold = 0.15  # 15% performance drop
        
    def check_retrain_trigger(self):
        """Decide if retraining needed"""
        # Performance degradation
        # Significant drift detected
        # New data available
        
    def retrain_and_validate(self):
        """Automated retraining pipeline"""
        # Train new model
        # A/B test against current
        # Rollback if worse
        # Deploy if better
```

**Deliverables**:
- âœ… Drift detection system
- âœ… Performance monitoring
- âœ… Automated retraining pipeline

---

## Implementation Schedule

### Week 1: Foundation
- **Day 1-3**: Data Pipeline (Ingestion + Validation)
- **Day 4-7**: Feature Engineering

### Week 2: ML Core  
- **Day 8-10**: Model Training & Registry
- **Day 11-14**: Model Serving

### Week 3: Intelligence
- **Day 15-17**: Monitoring System
- **Day 18-20**: Auto-Adaptation

### Week 4: Production Ready
- **Day 21-23**: Testing & Documentation
- **Day 24-25**: Performance Optimization
- **Day 26-28**: Deployment & Handover

---

## Key Design Decisions

### 1. Why Random Forest?
- Handles non-linear patterns well
- Less prone to overfitting
- Feature importance visibility
- Fast inference

### 2. Why Feature Store?
- Consistency between training and serving
- Reusability across models
- Version control for features
- Faster experimentation

### 3. Why Time-Based Splits?
- Prevents future data leakage
- Realistic validation
- Mimics production scenario

### 4. Why Automated Retraining?
- Adapts to market changes
- Prevents model staleness
- Reduces manual intervention

---

## Success Metrics

### Phase 1 (Data Pipeline)
- âœ… Data ingestion uptime: >99%
- âœ… Data validation rate: >95%
- âœ… Schema consistency: 100%

### Phase 2 (Features)
- âœ… Feature computation time: <1 second
- âœ… Feature coverage: >90% of data points
- âœ… Label accuracy: Manual validation of 100 samples

### Phase 3 (Training)
- âœ… Model accuracy: >85%
- âœ… Beats baseline by: >10%
- âœ… Training time: <5 minutes

### Phase 4 (Serving)
- âœ… API latency: <100ms
- âœ… Prediction availability: >99%
- âœ… Dashboard uptime: >99%

### Phase 5 (Monitoring)
- âœ… Drift detection lag: <1 hour
- âœ… Auto-retrain success: >90%
- âœ… Model improvement: Continuous

---

## Risk Mitigation

1. **Data Quality Issues**
   - Solution: Comprehensive validation rules
   - Fallback: Use previous valid data

2. **Model Overfitting**
   - Solution: Cross-validation, regularization
   - Fallback: Simpler model

3. **System Failures**
   - Solution: Redundancy, error handling
   - Fallback: Manual intervention

4. **Regime Monoculture**
   - Solution: Diversity enforcement
   - Fallback: Rule-based override

5. **Performance Degradation**
   - Solution: Auto-rollback
   - Fallback: Previous model version